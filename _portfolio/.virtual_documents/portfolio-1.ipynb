

















# Import the necessary libraries from the transformers package
from transformers import logging, AutoTokenizer, AutoModelForSeq2SeqLM

# Set logging level to warning to reduce verbosity
logging.set_verbosity(logging.WARNING)





# Load the car reviews dataset
import pandas as pd

# Specify the file path to the dataset
file_path = "data/car_reviews.csv"

# Read the dataset into a DataFrame
df = pd.read_csv(file_path, delimiter=";")

# Display the first few rows of the DataFrame
df.head()








# Running the sentiment classification

# Load a sentiment analysis LLM into a pipeline
from transformers import pipeline

classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')

# Perform inference on the car reviews and display prediction results
reviews = df.Review.to_list()
real_labels = df.Class.to_list()
predicted_labels = classifier(reviews)
for review, prediction, label in zip(reviews, predicted_labels, real_labels):
    print(f"Review: {review}\nActual Sentiment: {label}\nPredicted Sentiment: {prediction['label']} (Confidence: {prediction['score']:.4f})\n")

# Load accuracy and F1 score metrics
import evaluate
accuracy = evaluate.load('accuracy')
f1 = evaluate.load('f1')

# Map categorical sentiment labels into integer labels
references = [1 if label == "POSITIVE" else 0 for label in real_labels]
predictions = [1 if label['label'] == "POSITIVE" else 0 for label in predicted_labels]

# Calculate accuracy and F1 score
accuracy_result_dict = accuracy.compute(references=references, predictions=predictions)
accuracy_result = accuracy_result_dict['accuracy']
f1_result_dict = f1.compute(references=references, predictions=predictions)
f1_result = f1_result_dict['f1']
print(f"Accuracy: {accuracy_result}")
print(f"F1 result: {f1_result}")








# Load model directly
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-es-en", truncate=True)

# Filtering the data to be translated
data_to_translate = df.Review.iloc[0]
data_to_translate

# Selecting only the first two sentences
sentences = data_to_translate.split(".")
first_two_sentences = sentences[:2]
data_to_translate = ".".join(first_two_sentences)

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")
translated_review = translator(data_to_translate)[0]['translation_text']

# Loading BLEU metric
bleu = evaluate.load('bleu')

# Loading the reference translations
reviews_es = pd.read_csv('data/reference_translations.txt', names=['reviews'])
reviews_es = list(reviews_es.reviews.values)

# Calculating BLEU score
for review in reviews_es:
    print(f"Translated: {translated_review}")
    print(f"Original: {review}")
    bleu_score = bleu.compute(references=[review], predictions=[translated_review])
    print(f"Bleu Score: {bleu_score['bleu']}")
    print()








# Loading the model for QA
import transformers
model_name_QA = "deepset/minilm-uncased-squad2"

model = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name_QA)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_QA)

# Defining the question
question = "What did he like about the brand?"

# Selecting the 2nd review in the dataset
context = df['Review'][1]

# Tokenizing the inputs and returning PyTorch tensors
import torch
inputs = tokenizer.encode(question, context, return_tensors='pt')

with torch.no_grad():
    output = model(inputs)

# Catching the highest logits, which means the highest probability of the tokens for the beginning and for the end
start = torch.argmax(output.start_logits)
end = torch.argmax(output.end_logits) + 1

# Filtering the tokenized text for the highest probable tokens
answer_span = inputs[0][start:end]

# Translating the answer for human-readable format
answer = tokenizer.decode(answer_span)
print(f"Main terms at the answer = {answer}")








# Load the pre-trained tokenizer and model for text summarization
tokenizer = AutoTokenizer.from_pretrained("Falconsai/text_summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("Falconsai/text_summarization")

# Extract the last review text from the DataFrame
text = df['Review'].iloc[-1]

# Tokenize the input text and convert it to tensor format
inputs = tokenizer.encode(text, return_tensors='pt')

# Generate the summary using the model
outputs = model.generate(inputs, max_length=50)

# Decode the generated summary to a human-readable format
summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Display the summarized text
summarized_text






